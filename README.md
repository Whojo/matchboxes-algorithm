Inspired by Brillant's course on "Introduction to Neural Networks"


The matchbox algorithm aims to solve the tic-tac-toe game with a Neural Networks approach.


# Algorithm
## Construction of the graph
### Generation
It is generated by an approach closed to the brute force.
Firstly, we create a matrix of the correct size (aka 19.693x19.693).

We then start with the empty board.
We generate all possible moves.
Every possible moves represent the outgoing neighbours of the current node (aka the current board).
Those outgoing neighbours are saved in the matrix.
And we continue that way recursively until every possible (and accessible) boards have been added.

By default, the graph is generated at every call.
But you can save it that way to load it on later call.
``` sh
./src/main.py --save-nodes ./resources/unweighted_nodes.dot
```

### Load nodes
To prevent the (costly) generation of the graph, you can simply load it that way.

``` sh
./src/main.py --load-nodes ./resources/unweighted_nodes.dot
```
    
## Learning process
### How the algorithm is playing
When the algorithm is asked to choose a move, he randomly select an edge by considering the probabilties that each one has (aka their weight).


### Define edges' weight
The learning process is done in a similar way than neural network and is the whole point of this project.

Every edges in the graph is attributed to an arbitrary value at the initial phase (see more in #Experimentation).
After each iteration (that is after each game played), these edges are slightly modified to improve the performance of the algorithm.
This method should converge pretty slowly but still converge.

More precisely, when a game is being played, edges used are saved.
- If the game is won, those edges' weight are increase by 2.
- If the game is a draw, those edges' weight are increase by only 1.
- If the game is lost, those edges' weight are decrease by 1.

In this way, the probability a winning path is increased after each iteration and thus the performance of the algorithm,

The measurement of performance is the sum of all the weights.
This one should increase if it is learning.
When over a certain number of iterations (see more in #Experimentation), this measurement has not decrease, we can say that the algorithm has successfuly learned (and mastered) the game.

By default, the learning phase is executed at every call.
But you can save it that way to load it on later call.
``` sh
./src/main.py --save-nodes-and-edges ./resources/weighted_nodes.dot
```

    
### Load nodes and edges
To prevent the (costly) learning of the weights, you can simply load it that way.

``` sh
./src/main.py --load-nodes-and-edges ./resources/weighted_nodes.dot
```

    
## Play
During the learning process, the algorithm plays against a bot playing at random.
But now you can use the command line to play against it !

Enter this command and follow the instructions:
``` sh
./src/main.py
```

# Representation of boards
+---+---+---+
|   | x | o |
+---+---+---+
| x | x | o |
+---+---+---+
|   |   |   |
+---+---+---+

This board is represented this way:
" xoxxo   "

Each case is represented by one index and its label is then put in the index at this precise index
+---+---+---+
| 1 | 2 | 3 |
+---+---+---+
| 4 | 5 | 6 |
+---+---+---+
| 7 | 8 | 9 |
+---+---+---+


## Improve:
- We do not need to represent every boards. This is due to the fact that most of them are similar at 1 rotation/ symmetrie away. This reprensents a huge difference actually. The current model has 3^9 = 19 693 nodes. And we could reduce it to 304 nodes if we represent it the correct way.

Moreover, the overcost is only made in the construction of the graph. So the changement won't be that much of a pain.

- bitboard representation (is it possible on a three states as here ?)


# Experimentation
[TODO]

Initial nbr of matches
Successful learning threshold

     
# Dependencies
[TODO]
Numpy

# TLDR; How to use it ?
[TODO]
